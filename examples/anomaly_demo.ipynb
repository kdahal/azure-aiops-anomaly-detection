{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa9c500",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies\n",
    "Run this once to load project libs (requests, pandas, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a36290b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests==2.31.0 (from -r ../requirements.txt (line 1))\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting azure-monitor-query==1.2.0 (from -r ../requirements.txt (line 2))\n",
      "  Using cached azure_monitor_query-1.2.0-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting azure-identity==1.17.0 (from -r ../requirements.txt (line 3))\n",
      "  Using cached azure_identity-1.17.0-py3-none-any.whl.metadata (79 kB)\n",
      "Collecting pandas==2.1.4 (from -r ../requirements.txt (line 4))\n",
      "  Using cached pandas-2.1.4.tar.gz (4.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "‚úÖ Deps installed‚Äîrestart kernel if needed (Kernel > Restart).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  √ó pip subprocess to install build dependencies did not run successfully.\n",
      "  ‚îÇ exit code: 1\n",
      "  ‚ï∞‚îÄ> [55 lines of output]\n",
      "      Ignoring oldest-supported-numpy: markers 'python_version < \"3.12\"' don't match your environment\n",
      "      Collecting meson-python==0.13.1\n",
      "        Using cached meson_python-0.13.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "      Collecting meson==1.2.1\n",
      "        Using cached meson-1.2.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "      Collecting wheel\n",
      "        Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "      Collecting Cython<3,>=0.29.33\n",
      "        Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)\n",
      "      Collecting numpy<2,>=1.26.0\n",
      "        Using cached numpy-1.26.4.tar.gz (15.8 MB)\n",
      "        Installing build dependencies: started\n",
      "        Installing build dependencies: finished with status 'done'\n",
      "        Getting requirements to build wheel: started\n",
      "        Getting requirements to build wheel: finished with status 'done'\n",
      "        Installing backend dependencies: started\n",
      "        Installing backend dependencies: finished with status 'done'\n",
      "        Preparing metadata (pyproject.toml): started\n",
      "        Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "        error: subprocess-exited-with-error\n",
      "      \n",
      "        √É‚Äî Preparing metadata (pyproject.toml) did not run successfully.\n",
      "        √¢‚Äù‚Äö exit code: 1\n",
      "        √¢‚Ä¢¬∞√¢‚Äù‚Ç¨> [21 lines of output]\n",
      "            + c:\\Users\\kumar\\AppData\\Local\\Programs\\Python\\Python313\\python.exe C:\\Users\\kumar\\AppData\\Local\\Temp\\pip-install-kbu5l7ou\\numpy_d48f3e5f31d64b2088ff3c0227b9c005\\vendored-meson\\meson\\meson.py setup C:\\Users\\kumar\\AppData\\Local\\Temp\\pip-install-kbu5l7ou\\numpy_d48f3e5f31d64b2088ff3c0227b9c005 C:\\Users\\kumar\\AppData\\Local\\Temp\\pip-install-kbu5l7ou\\numpy_d48f3e5f31d64b2088ff3c0227b9c005\\.mesonpy-ro1ku3it -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\kumar\\AppData\\Local\\Temp\\pip-install-kbu5l7ou\\numpy_d48f3e5f31d64b2088ff3c0227b9c005\\.mesonpy-ro1ku3it\\meson-python-native-file.ini\n",
      "            The Meson build system\n",
      "            Version: 1.2.99\n",
      "            Source dir: C:\\Users\\kumar\\AppData\\Local\\Temp\\pip-install-kbu5l7ou\\numpy_d48f3e5f31d64b2088ff3c0227b9c005\n",
      "            Build dir: C:\\Users\\kumar\\AppData\\Local\\Temp\\pip-install-kbu5l7ou\\numpy_d48f3e5f31d64b2088ff3c0227b9c005\\.mesonpy-ro1ku3it\n",
      "            Build type: native build\n",
      "            Project name: NumPy\n",
      "            Project version: 1.26.4\n",
      "            WARNING: Failed to activate VS environment: Could not find C:\\Program Files (x86)\\Microsoft Visual Studio\\Installer\\vswhere.exe\n",
      "      \n",
      "            ..\\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]\n",
      "            The following exception(s) were encountered:\n",
      "            Running `icl \"\"` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "            Running `cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "            Running `cc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "            Running `gcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "            Running `clang --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "            Running `clang-cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "            Running `pgcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      \n",
      "            A full log can be found at C:\\Users\\kumar\\AppData\\Local\\Temp\\pip-install-kbu5l7ou\\numpy_d48f3e5f31d64b2088ff3c0227b9c005\\.mesonpy-ro1ku3it\\meson-logs\\meson-log.txt\n",
      "            [end of output]\n",
      "      \n",
      "        note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "      error: metadata-generation-failed\n",
      "      \n",
      "      √É‚Äî Encountered error while generating package metadata.\n",
      "      √¢‚Ä¢¬∞√¢‚Äù‚Ç¨> See above for output.\n",
      "      \n",
      "      note: This is an issue with the package mentioned above, not pip.\n",
      "      hint: See above for details.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "√ó pip subprocess to install build dependencies did not run successfully.\n",
      "‚îÇ exit code: 1\n",
      "‚ï∞‚îÄ> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "# Install from requirements.txt (run once)\n",
    "%pip install -r ../requirements.txt\n",
    "print('‚úÖ Deps installed‚Äîrestart kernel if needed (Kernel > Restart).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d224be50",
   "metadata": {},
   "source": [
    "# AIOps Anomaly Detection: Interactive Demo\n",
    "\n",
    "**Overview**: Offline simulation of automated anomaly detection for IT ops (e.g., Azure VM CPU metrics). \n",
    "- Learns 'normal' patterns from time-series data.\n",
    "- Flags deviations without manual thresholds.\n",
    "- Visualizes actual vs. expected for proactive downtime prevention.\n",
    "\n",
    "**Tune & Re-run**: Change `anomaly_prob` or `num_points` in Cell 3, then re-execute.\n",
    "\n",
    "Future: Replace mocks with Azure Monitor fetches for real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4281e5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: ANOMALY_DETECTOR_ENDPOINT=https://anomalydetector-vtmarvj2flhgk.cognitiveservices.azure.com/\n",
      "env: ANOMALY_DETECTOR_KEY=d7d7498c94d4461fbe5a73a70b7a3330\n"
     ]
    }
   ],
   "source": [
    "# Imports (path to root for src/)\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()  # Disable interactive mode (stops empty windows)\n",
    "import sys\n",
    "import random\n",
    "sys.path.append('..')\n",
    "from src.detect_anomalies import create_payload_with_anomalies\n",
    "from src.visualize_anomalies import plot_detection\n",
    "from datetime import datetime\n",
    "%env ANOMALY_DETECTOR_ENDPOINT=https://anomalydetector-vtmarvj2flhgk.cognitiveservices.azure.com/\n",
    "%env ANOMALY_DETECTOR_KEY=d7d7498c94d4461fbe5a73a70b7a3330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef06ad51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 50 points with ~7 expected anomalies.\n",
      "Sample data: [{'timestamp': '2025-10-09T20:25:00Z', 'value': 79.19476742385832}, {'timestamp': '2025-10-09T20:26:00Z', 'value': 78.00571896127943}, {'timestamp': '2025-10-09T20:27:00Z', 'value': 84.78626478630558}]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Generate synthetic time-series data\n",
    "# Simulates metrics like CPU % with patterns + random anomalies (spikes/drops)\n",
    "random.seed(42)  # Reproducible for demos\n",
    "payload = create_payload_with_anomalies(\n",
    "    num_points=50,  # e.g., 50 mins of data\n",
    "    anomaly_prob=0.15,  # 15% deviation chance (tune here!)\n",
    "    base_value=75  # Normal baseline\n",
    ")\n",
    "print(f\"Generated {len(payload['series'])} points with ~{int(0.15*50)} expected anomalies.\")\n",
    "print(\"Sample data:\", payload['series'][:3])  # Peek at first points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6742c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mock detection: 9 anomalies flagged! üö®\n",
      "(Proactive alert: Auto-scale resources?)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Mock Detection (AIOps 'learns' patterns & flags deviations)\n",
    "# Real: Call Azure Anomaly Detector API with env vars\n",
    "result = {\n",
    "    'isAnomaly': True,  # Overall flag\n",
    "    'expectedValues': [75] * len(payload['series']),  # Mock learned baseline\n",
    "    'anomalyStatus': [1 if random.random() < 0.15 else 0 for _ in payload['series']]  # Per-point flags\n",
    "}\n",
    "anomaly_count = sum(result['anomalyStatus'])\n",
    "print(f\"Mock detection: {anomaly_count} anomalies flagged! üö®\\n(Proactive alert: Auto-scale resources?)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c95cd99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug: Payload series length: 50\n",
      "Debug: Sample values: [79.19476742385832, 78.00571896127943, 84.78626478630558, 83.62510800075151, 81.08970211438172]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kumar\\azure-aiops-anomaly-detection\\examples\\..\\src\\visualize_anomalies.py:35: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  'time': pd.to_datetime(timestamps),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Plot saved to anomaly_demo_nb.png\n",
      "Plot rendered‚Äîcheck inline/PNG for spikes & reds!\n",
      "üì∏ Timestamped plot saved: anomaly_demo_20251009_211403.png (check file size >50KB for content)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Visualize (actual vs. expected, red dots on deviations)\n",
    "import matplotlib.pyplot as plt  # For post-plot saves\n",
    "\n",
    "# Debug: Print payload to confirm data exists\n",
    "print(\"Debug: Payload series length:\", len(payload['series']))\n",
    "print(\"Debug: Sample values:\", [p['value'] for p in payload['series'][:5]])\n",
    "\n",
    "plot_detection(payload, result, save_path='anomaly_demo_nb.png', show_plot=False)  # No plt.show() = no pop-up\n",
    "print(\"Plot rendered‚Äîcheck inline/PNG for spikes & reds!\")\n",
    "\n",
    "# Tweak: Auto-save timestamped high-res PNG to repo (for commits)\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "save_name = f\"anomaly_demo_{timestamp}.png\"\n",
    "plt.savefig(save_name, dpi=150, bbox_inches='tight', facecolor='white')  # Saves open figure\n",
    "print(f\"üì∏ Timestamped plot saved: {save_name} (check file size >50KB for content)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97610d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug: Endpoint: \n",
      "Debug: Key: None\n",
      "‚ö†Ô∏è Env vars missing‚Äîstaying in mock mode.\n",
      "Detection: 8 anomalies flagged! üö®\n"
     ]
    }
   ],
   "source": [
    "# Step 2.5: Live Azure API Test (Optional‚ÄîUse Real Detection)\n",
    "# Set env vars in terminal, then toggle for live run\n",
    "use_real_api = True  # Set True to call real Anomaly Detector (requires endpoint/key)\n",
    "\n",
    "import os  # Import for getenv\n",
    "\n",
    "# Set env vars here for Jupyter kernel (overrides terminal if needed)\n",
    "os.environ[\"ANOMALY_DETECTOR_ENDPOINT\"] = \"\"\n",
    "os.environ[\"ANOMALY_DETECTOR_KEY\"] = \"\"\n",
    "\n",
    "if use_real_api:\n",
    "    endpoint = os.getenv(\"ANOMALY_DETECTOR_ENDPOINT\")\n",
    "    api_key = os.getenv(\"ANOMALY_DETECTOR_KEY\")\n",
    "    print(f\"Debug: Endpoint: {endpoint}\")  # Full for verification\n",
    "    if api_key:  # Check before slicing\n",
    "        print(f\"Debug: Key (partial): {api_key[:10]}...\")  # Partial for security\n",
    "    else:\n",
    "        print(\"Debug: Key: None\")\n",
    "    if endpoint and api_key:\n",
    "        from src.detect_anomalies import detect_via_api\n",
    "        result = detect_via_api(payload, endpoint, api_key)  # Real call\n",
    "        print(\"üî¥ Live Azure API: Real detection results!\")\n",
    "        print(f\"API Response: isAnomaly={result.get('isAnomaly')}, expectedChangePoint={result.get('expectedChangePoint')}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Env vars missing‚Äîstaying in mock mode.\")\n",
    "        # Fallback to mock\n",
    "        result = {\n",
    "            'isAnomaly': True,\n",
    "            'expectedValues': [75] * len(payload['series']),\n",
    "            'anomalyStatus': [1 if random.random() < 0.15 else 0 for _ in payload['series']]  # Mock uses this\n",
    "        }\n",
    "else:\n",
    "    print(\"üß™ Mock mode: Simulated detection.\")\n",
    "    result = {\n",
    "        'isAnomaly': True,\n",
    "        'expectedValues': [75] * len(payload['series']),\n",
    "        'anomalyStatus': [1 if random.random() < 0.15 else 0 for _ in payload['series']]\n",
    "    }\n",
    "\n",
    "# Fixed: Count True in 'isAnomaly' list (real API) or 1 in 'anomalyStatus' (mock)\n",
    "if 'isAnomaly' in result and isinstance(result['isAnomaly'], list):\n",
    "    anomaly_count = sum(1 for x in result['isAnomaly'] if x)  # Real: Count True bools\n",
    "else:\n",
    "    anomaly_count = sum(result.get('anomalyStatus', [0] * len(payload['series'])))  # Mock fallback\n",
    "print(f\"Detection: {anomaly_count} anomalies flagged! üö®\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62b7d55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üö® AIOps Alert: High Deviation Detected!\n",
      "- Metrics: CPU % (simulated)\n",
      "- Anomalies: 8 in last 50 mins\n",
      "- Top Deviation: Max spike 111.84357447768423% at ~2025-10-09T20:25:00Z\n",
      "- Action: Auto-scale resources or investigate logs.\n",
      "- Severity: Medium (tune for multi-cloud)\n",
      "\n",
      "üìß Mock: Alert sent to Ops team via Azure Alerts/Teams.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Mock Alerting (Proactive Response on Anomalies)\n",
    "# Real: Post to Azure Action Group webhook or email\n",
    "alert_threshold = 3  # e.g., alert if >3 anomalies in window\n",
    "if anomaly_count > alert_threshold:\n",
    "    alert_msg = f\"\"\"\n",
    "üö® AIOps Alert: High Deviation Detected!\n",
    "- Metrics: CPU % (simulated)\n",
    "- Anomalies: {anomaly_count} in last {len(payload['series'])} mins\n",
    "- Top Deviation: Max spike {max([p['value'] for p in payload['series']])}% at ~{payload['series'][0]['timestamp']}\n",
    "- Action: Auto-scale resources or investigate logs.\n",
    "- Severity: Medium (tune for multi-cloud)\n",
    "\"\"\"\n",
    "    print(alert_msg)\n",
    "    \n",
    "    # Mock 'send' (e.g., to email/Slack)\n",
    "    print(\"üìß Mock: Alert sent to Ops team via Azure Alerts/Teams.\")\n",
    "else:\n",
    "    print(\"‚úÖ No alerts triggered‚Äîall within normal patterns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9afde3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚òÅÔ∏è Using AWS CloudWatch mock‚Äîreal boto3 swaps in here!\n",
      "Payload ready: 50 points from 'AWS'.\n",
      "AWS Mock detection: 7 anomalies flagged! üö®\n",
      "üìä Plot saved to aws_anomaly_demo.png\n",
      "AWS plot rendered‚Äîcompare spikes to Azure!\n"
     ]
    }
   ],
   "source": [
    "# Step 4.5: AWS CloudWatch Mock (Multi-Cloud Tease)\n",
    "# Real: Use boto3.client('cloudwatch').get_metric_statistics() for EC2 CPU\n",
    "def fetch_cloudwatch_mock(instance_id=\"i-mock123\", num_points=50):\n",
    "    \"\"\"\n",
    "    Mocks CloudWatch query: e.g., CPUUtilization for EC2 instance over 50 mins.\n",
    "    Returns payload-ready series.\n",
    "    \"\"\"\n",
    "    import random\n",
    "    from datetime import timedelta\n",
    "    base_ts = datetime.now()\n",
    "    series = []\n",
    "    for i in range(num_points):\n",
    "        ts = (base_ts - timedelta(minutes=i)).isoformat()\n",
    "        value = 70 + random.uniform(-8, 12) + (i % 8)  # Simulated CPU with trend\n",
    "        if random.random() < 0.12:  # 12% anomaly\n",
    "            value += random.uniform(15, 45)  # Burst\n",
    "        series.append({\"timestamp\": ts, \"value\": value})\n",
    "    return {\"series\": series, \"granularity\": \"PT1M\"}\n",
    "\n",
    "# Toggle for AWS mode\n",
    "use_aws_mock = True  # Set False for Azure/synthetic\n",
    "if use_aws_mock:\n",
    "    payload = fetch_cloudwatch_mock(num_points=50)\n",
    "    print(\"‚òÅÔ∏è Using AWS CloudWatch mock‚Äîreal boto3 swaps in here!\")\n",
    "else:\n",
    "    # Fall back\n",
    "    payload = fetch_metrics_mock(num_points=50)  # From Azure stub\n",
    "    print(\"‚òÅÔ∏è Using Azure stub.\")\n",
    "\n",
    "print(f\"Payload ready: {len(payload['series'])} points from 'AWS'.\")\n",
    "\n",
    "# Re-run detection & viz for comparison (quick re-execute)\n",
    "result = {\n",
    "    'isAnomaly': True,\n",
    "    'expectedValues': [70] * len(payload['series']),  # AWS baseline tweak\n",
    "    'anomalyStatus': [1 if random.random() < 0.12 else 0 for _ in payload['series']]\n",
    "}\n",
    "anomaly_count = sum(result['anomalyStatus'])\n",
    "print(f\"AWS Mock detection: {anomaly_count} anomalies flagged! üö®\")\n",
    "\n",
    "plot_detection(payload, result, save_path='aws_anomaly_demo.png', show_plot=True)\n",
    "print(\"AWS plot rendered‚Äîcompare spikes to Azure!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d56553b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Using GCP Operations Suite mock‚Äîreal SDK swaps in here!\n",
      "Payload ready: 50 points from 'GCP'.\n",
      "GCP Mock detection: 5 anomalies flagged! üö®\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kumar\\azure-aiops-anomaly-detection\\examples\\..\\src\\visualize_anomalies.py:35: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  'time': pd.to_datetime(timestamps),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Plot saved to gcp_anomaly_demo.png\n",
      "GCP plot rendered‚Äîcompare to Azure/AWS!\n"
     ]
    }
   ],
   "source": [
    "# Step 4.75: GCP Operations Suite Mock (Full Multi-Cloud Trifecta)\n",
    "# Real: Use google-cloud-monitoring SDK for VM instance metrics\n",
    "def fetch_gcp_mock(project_id=\"my-project-123\", num_points=50):\n",
    "    \"\"\"\n",
    "    Mocks GCP Operations Suite query: e.g., CPU utilization for Compute Engine VM over 50 mins.\n",
    "    Returns payload-ready series.\n",
    "    \"\"\"\n",
    "    import random\n",
    "    from datetime import timedelta\n",
    "    base_ts = datetime.now()\n",
    "    series = []\n",
    "    for i in range(num_points):\n",
    "        ts = base_ts - timedelta(minutes=i)\n",
    "        timestamp = ts.replace(second=0, microsecond=0).isoformat() + 'Z'  # Minute-aligned UTC\n",
    "        value = 72 + random.uniform(-9, 11) + (i % 6)  # Simulated CPU with seasonality\n",
    "        if random.random() < 0.11:  # 11% anomaly\n",
    "            value += random.uniform(18, 42)  # Surge\n",
    "        series.append({\"timestamp\": timestamp, \"value\": value})\n",
    "    series.reverse()  # Ascending order\n",
    "    return {\"series\": series, \"granularity\": \"minutely\"}\n",
    "\n",
    "# Toggle for GCP mode\n",
    "use_gcp_mock = True  # Set False for Azure/AWS/synthetic\n",
    "if use_gcp_mock:\n",
    "    payload = fetch_gcp_mock(num_points=50)\n",
    "    print(\"üåê Using GCP Operations Suite mock‚Äîreal SDK swaps in here!\")\n",
    "else:\n",
    "    # Fall back to AWS\n",
    "    payload = fetch_cloudwatch_mock(num_points=50)\n",
    "    print(\"üåê Using AWS mock.\")\n",
    "\n",
    "print(f\"Payload ready: {len(payload['series'])} points from 'GCP'.\")\n",
    "\n",
    "# Re-run detection & viz for comparison\n",
    "result = {\n",
    "    'isAnomaly': True,\n",
    "    'expectedValues': [72] * len(payload['series']),  # GCP baseline tweak\n",
    "    'anomalyStatus': [1 if random.random() < 0.11 else 0 for _ in payload['series']]\n",
    "}\n",
    "anomaly_count = sum(result['anomalyStatus'])\n",
    "print(f\"GCP Mock detection: {anomaly_count} anomalies flagged! üö®\")\n",
    "\n",
    "plot_detection(payload, result, save_path='gcp_anomaly_demo.png', show_plot=True)\n",
    "print(\"GCP plot rendered‚Äîcompare to Azure/AWS!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63174858",
   "metadata": {},
   "source": [
    "| **GCP** | Operations Suite | Alerting policies to Pub/Sub. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25c9f8a",
   "metadata": {},
   "source": [
    "## Multi-Cloud Extension + Alerting\n",
    "\n",
    "| Cloud | Data Source | Alert Integration |\n",
    "|-------|-------------|-------------------|\n",
    "| **Azure** | Monitor/Log Analytics | Action Groups + Logic Apps for emails/SMS. |\n",
    "| **AWS** | CloudWatch | SNS notifications on alarms. |\n",
    "| **GCP** | Operations Suite | Alerting policies to Pub/Sub. |\n",
    "\n",
    "**Next Steps**: \n",
    "- Wire mocks to real webhooks.\n",
    "- Test with simulated downtime (e.g., anomaly_prob=0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ddfeb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webhook failed: name 'requests' is not defined (staying in mock mode)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Mock Webhook Alert (Real Notification Tease)\n",
    "# Real: Post to Azure Logic App webhook or Slack/Teams URL\n",
    "webhook_url = \"https://webhook.site/1234-5678-90ab-cdef\"  # Test URL (sign up at webhook.site for temp endpoint)\n",
    "# Or real: \"https://prod-00.eastus.logic.azure.com/workflows/abc123/triggers/manual/paths/invoke?sp=%2Ftriggers%2Fmanual%2Frun&sv=1.0&sig=xyz\"\n",
    "\n",
    "if anomaly_count > alert_threshold:\n",
    "    alert_data = {\n",
    "        \"title\": \"AIOps Anomaly Alert\",\n",
    "        \"message\": alert_msg,\n",
    "        \"anomalies\": anomaly_count,\n",
    "        \"max_spike\": max([p['value'] for p in payload['series']]),\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(webhook_url, json=alert_data, headers={\"Content-Type\": \"application/json\"})\n",
    "        if response.status_code == 200:\n",
    "            print(\"üìß Webhook sent successfully! Check your endpoint for the alert payload.\")\n",
    "            print(f\"Response: {response.json() if response.content else 'OK'}\")\n",
    "        else:\n",
    "            print(f\"Webhook error: {response.status_code} - {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Webhook failed: {e} (staying in mock mode)\")\n",
    "else:\n",
    "    print(\"No webhook needed‚Äîno anomalies above threshold.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
